
# 相似度检索demo

## 环境要求

需要安装：
1. `faiss`，用于建立索引，有cpu（`faiss-cpu`）和gpu（`faiss-gpu`）两种版本，gpu版本需有 CUDA 环境（需要 NVIDIA 显卡 + 安装好 CUDA 驱动，可能需要和你本地的 CUDA 版本匹配，否则建议用 conda 安装），这里我使用cpu版本。
2. `sentence_transformers`，用于生成向量。

方式一：直接使用pip安装，可能在许多机子上有兼容性问题，不行可以使用conda安装
```shell
# 创建python虚拟环境
python3 -m venv .venv
source .venv/bin/activate
pip3 install -r ./requirements.txt 
```

方式二：有些机子用 `pip` 直接安装 `paiss` 会各种报错，可以使用 [conda](https://www.anaconda.com/docs/getting-started/miniconda/main) 安装

```shell
# 创建一个干净的环境，命名为 faiss
conda create -n faiss -y
# 激活这个环境
conda activate faiss
# 安装 CPU 版本
conda install -c pytorch python=3.8 faiss-cpu -y
# 安装 sentence_transformers
# 注意先激活conda环境，激活后会安装在conda环境中，否则会安装到系统全局中。
pip3 install sentence_transformers
```

PS：如果想删除已经创建的conda环境

```shell
conda remove -n faiss --all
```

## 任务执行

第一步：预处理数据。执行 `pre_process_data.py`，清理空格，划分句子。文件生成在tmp文件夹，不依赖任何三方库。预处理数据的方式直接影响搜索效果。这里实现了 `SimpleProcessor` 和 `WindowProcessor` 两种方式。

第二步：计算向量并保存到本地。执行 `gen_vectors.py`。依赖 `sentence_transformers` 库。这里我们使用 [uer/sbert-base-chinese-nli](https://huggingface.co/uer/sbert-base-chinese-nli) 模型，`sentence-transformers` 会自动从 [HuggingFace Hub](https://huggingface.co) 下载模型。如想下载到本地使用，可以先自动下载，然后去本地缓存目录(~/.cache/huggingface/transformers)下寻找，并指定加载本地模型文件夹。这里同样保存了 `SimpleProcessor` 和 `WindowProcessor` 两种预处理数据的向量。

第三步：使用 `faiss` 实现最简单的向量检索功能平面索引，依赖 `faiss-cpu`，同样对两种不同的预处理方式进行了查找：
1. 平面索引(IndexFlatL2)：执行 `IndexFlatL2.py`
2. 分区索引(IndexIVFlatL2)：执行 `IndexIVFlatL2.py`
3. 乘积量化(IndexIVFPQ)：执行 `IndexIVFPQ.py`

其他：
我们这里使用的[uer/sbert-base-chinese-nli](https://huggingface.co/uer/sbert-base-chinese-nli) 模型适合用于语义相似度检索，对于提问形式的搜索并不友好，如果想加强提问形式的回答效果，可能要考虑使用其他支持问答模式的模型，如[BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)、[shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)


## 文本预处理优化
`pre_process_data.py`中，简单的清理空格，划分句子的方法效果一般。于是后面又写了 `windowProcessor` 来优化。不过最后看起来效果差不多。

|划分方式|适用场景|优缺点|
|---|---|---|
|按段落分（每段为一个向量）|小说、文档、上下文重要|保留语义完整，但粒度略大|
|按句子分（每句为一个向量）|FAQ、语义对齐|粒度细，但上下文丢失|
|滑动窗口（句子拼接、重叠）|技术文档、QA系统|控制长度 & 保留上下文，推荐 ✅|

优化选择滑动窗口方案。处理结构建议：
1. 清理无用内容
    - 去除无效符号（如多余空格、换行、制表符等）
    - 可保留标点，特别是中文中的 “。！？”，有助断句
    - 去除过短或无意义句子（如 <章节标题> 或数字）
2. 保留结构化信息
    - 每条文本片段（句子/段落）建议配上结构信息，如章节、页码、index等
    - 可以更好地展示检索结果（显示章节、页码）
    - 用于过滤检索范围（如只查第三章内容）
3. 控制每段长度（token数）
    - 不同 embedding 模型的最大支持长度不同
    - 建议每段文本不超过400 tokens（中文约200-300字），过长会导致向量模糊，影响相似度匹配

处理流程
1. 按段或换行拆分
2. 按句号、感叹号、问号等标点切分短句
3. 合并句子为window（每个 windown 3句，每次滑动2句）组成一个检索单元
4. 保留 metadata（如章节、页码、段号、人物）
5. 每个检索单元 encode 后存入向量数据库

## 效果

**搜索"小王子伤心"**

使用单纯按句号和换行符分割的文本预处理
```
Distances: [[226.14204 250.29906 256.32422 269.7577  281.6846 ]]
Indices: [[285 112 599 337 227]]
Index 285: Distance 226.1420440673828
sentence: 访问时间非常短，可是它却使小王子非常忧伤
---------------------
Index 112: Distance 250.29905700683594
sentence: 我在讲述这些往事时心情是很难过的
---------------------
Index 599: Distance 256.32421875
sentence: 我却很悲伤
---------------------
Index 337: Distance 269.7576904296875
sentence: “啊！”小王子大失所望
---------------------
Index 227: Distance 281.6846008300781
sentence: 他有点忧伤
---------------------
```

使用滑动窗口进行文本预处理
```
Distances: [[350.7876  354.0715  392.18158 402.57678 411.22946]]
Indices: [[ 83  51 259  80 138]]
Index 83: Distance 350.78759765625
sentence: “你说话就和那些大人一样！”这话使我有点难堪。 可是他又尖刻无情地说道：“你什么都分不清…你把什么都混在一起！”他着实非常恼火。 摇动着脑袋，金黄色的头发随风颤动着。
---------------------
Index 51: Distance 354.0715026855469
sentence: 我在讲述这些往事时心情是很难过的。 我的朋友带着他的小羊已经离去六年了。 我之所以在这里尽力把他描写出来，就是为了不要忘记他。
---------------------
Index 259: Distance 392.18157958984375
sentence: 我不知为什么，又感到一阵莫名其妙的心酸。 这时，我产生了一个问题：“一星期以前，我认识你的那天早上，你单独一个人在这旷无人烟的地方走着；这么说，这并不是偶然的了？你是要回到你降落的地方去是吗！”小王子的脸又红了。 我犹豫不定地又说了一句：“可能是因为周年纪念吧？…”小王子脸又红了。
---------------------
Index 80: Distance 402.5767822265625
sentence: 我发现机器故障似乎很严重，饮水也快完了，担心可能发生最坏的情况，心里很着急。 “那么刺有什么用呢？”小王子一旦提出了问题，从来不会放过。 这个该死的螺丝使我很恼火，我于是就随便回答了他一句：“刺么，什么用都没有，这纯粹是花的恶劣表现。”“噢！”可是他沉默了一会儿之后，怀着不满的心情冲我说：“我不信！花是弱小的、淳朴的，它们总是设法保护自己，以为有了刺就可以显出自己的厉害…”我默不作声。
---------------------
Index 138: Distance 411.2294616699219
sentence: 小王子已经有些可怜酒鬼。 他问道：“忘却什么呢？”酒鬼垂下脑袋坦白道：“为了忘却我的羞愧。”“你羞愧什么呢？”小王子很想救助他。 “我羞愧我喝酒。”酒鬼说完以后就再也不开口了。
---------------------
```

**搜索"看日落"**

使用单纯按句号和换行符分割的文本预处理

```
Distances: [[413.1753  416.4456  416.5908  417.78174 429.86734]]
Indices: [[151 153 154 251 172]]
Index 151: Distance 413.17529296875
sentence: 过去相当长的时间里你唯一的乐趣就是观赏那夕阳西下的温柔晚景
---------------------
Index 153: Distance 416.4455871582031
sentence: 你当时对我说道：“我喜欢看日落。我们去看一回日落吧！”“可是得等着…”“等什么？”“等太阳落山。”开始，你显得很惊奇的样子，随后你笑自己的糊涂
---------------------
Index 154: Distance 416.5907897949219
sentence: 你对我说：“我总以为是在我的家乡呢！”确实，大家都知道，在美国是正午时分，在法国，正夕阳西下，只要在一分钟内赶到法国就可看到日落
---------------------
Index 251: Distance 417.78173828125
sentence: 他遗憾没有看到日落
---------------------
Index 172: Distance 429.8673400878906
sentence: 夜幕已经降临
```

使用滑动窗口进行文本预处理
```
Distances: [[415.34167 427.9214  468.5822  529.93866 543.60156]]
Indices: [[ 75  76  74 251  95]]
Index 75: Distance 415.3416748046875
sentence: 过去相当长的时间里你唯一的乐趣就是观赏那夕阳西下的温柔晚景。 这个新的细节，是我在第四天早晨知道的。 你当时对我说道：“我喜欢看日落。我们去看一回日落吧！”“可是得等着…”“等什么？”“等太阳落山。”开始，你显得很惊奇的样子，随后你笑自己的糊涂。
---------------------
Index 76: Distance 427.92138671875
sentence: 你当时对我说道：“我喜欢看日落。我们去看一回日落吧！”“可是得等着…”“等什么？”“等太阳落山。”开始，你显得很惊奇的样子，随后你笑自己的糊涂。 你对我说：“我总以为是在我的家乡呢！”确实，大家都知道，在美国是正午时分，在法国，正夕阳西下，只要在一分钟内赶到法国就可看到日落。 可惜法国是那么的遥远。
---------------------
Index 74: Distance 468.58221435546875
sentence: VI啊！ 小王子，就这样，我逐渐懂得了你那忧郁的生活。 过去相当长的时间里你唯一的乐趣就是观赏那夕阳西下的温柔晚景。
---------------------
Index 251: Distance 529.9386596679688
sentence: 就象节日一般舒适愉快。 这水远不只是一种饮料，它是披星戴月走了许多路才找到的，是在辘轳的歌声中，经过我双臂的努力得来的。 它象是一件礼品慰藉着心田。
---------------------
Index 95: Distance 543.6015625
sentence: 是的，她是非常爱俏的。 她用好些好些日子天仙般地梳妆打扮。 然后，在一天的早晨，恰好在太阳升起的时候，她开放了。
---------------------
```

## 参考
[向量数据库入坑指南：聊聊来自元宇宙大厂 Meta 的相似度检索技术 Faiss](https://soulteary.com/2022/09/03/vector-database-guide-talk-about-the-similarity-retrieval-technology-from-metaverse-big-company-faiss.html)